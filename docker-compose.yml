version: '3.8'

services:
  # 1. Servicio de Ingesta (Músculo de procesamiento)
  ingestion-service:
    build: ./Ingest # Asegúrate que la ruta sea correcta a la carpeta Ingest
    ports:
      - "8000:8000"
    env_file: ./Ingest/.env
    environment:
      - OCR_ENDPOINT=http://localhost:9999/ocr
      - OCR_ENGINE_NAME=rapidocr

  # 2. Base de Datos (Memoria)
  neo4j-db:
    image: neo4j:5.15
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/prismafinance123
      - NEO4J_PLUGINS=["apoc"]
    volumes:
      - neo4j_data:/data

  # 3. LLM Local (Cerebro local)
  ollama-service:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama

  # 4. API Principal (Cerebro Orquestador - Sistema B + Lógica A)
  prisma-backend:
    build: . # Dockerfile en raíz que copia ./app
    ports:
      - "8081:8081" # Puerto expuesto a Open WebUI
    depends_on:
      - ingestion-service
      - neo4j-db
      - ollama-service
    environment:
      - NEO4J_URI=bolt://neo4j-db:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=prismafinance123
      - INGESTION_URL=http://ingestion-service:8000
      - OLLAMA_BASE_URL=http://ollama-service:11434/v1 # LangChain usa standard OpenAI format
      - LM_STUDIO_URL=http://ollama-service:11434/v1   # Reutilizamos Ollama como si fuera LM Studio

volumes:
  neo4j_data:
  ollama_storage: